# Taha Mansouri

Lecturer in AI at the University of Salford  
Computer Vision & Multimodal AI Â· LLMs Â· Responsible AI

<p align="left">
  <img src="https://img.shields.io/badge/Computer%20Vision-%26%20Multimodal%20AI-1f6feb" alt="Computer Vision and Multimodal AI" />
  <img src="https://img.shields.io/badge/LLMs-Reasoning%20%26%20RAG-8250df" alt="LLMs" />
  <img src="https://img.shields.io/badge/Responsible%20AI-Fairness%20%26%20Explainability-0e8a16" alt="Responsible AI" />
  <img src="https://img.shields.io/badge/University%20of%20Salford-Lecturer%20in%20AI-7A003C" alt="University of Salford" />
</p>

[University Profile](https://www.salford.ac.uk/our-staff/taha-mansouri) Â· [LinkedIn](https://www.linkedin.com/in/taha-mansouri-7969095a/) Â· [GitHub](https://github.com/tahamsi)

---

## ğŸ‘‹ About

I am a Lecturer in Artificial Intelligence at the University of Salford, working across research, teaching, and supervision. My work focuses on Computer Vision & Multimodal AI, LLMs, and Responsible AI, with an emphasis on systems that are explainable, robust, and suitable for real-world deployment. I combine academic research with industry-facing work, and I am particularly interested in trustworthy AI evaluation, multimodal systems, and AI to support education.

---

## ğŸ”¬ Research

<table>
  <tr>
    <td valign="top" width="33%">
      <h3>Computer Vision & Multimodal AI</h3>
      <ul>
        <li>
          <a href="https://github.com/tahamsi/facial_expression_detection">facial_expression_detection</a><br/>
          Video-based facial expression analysis and model-checking workflows.
        </li>
        <li>
          <a href="https://github.com/tahamsi/qwen-vl-caption-api">qwen-vl-caption-api</a><br/>
          Vision-language captioning experiments and API integration work.
        </li>
      </ul>
    </td>
    <td valign="top" width="33%">
      <h3>LLMs</h3>
      <ul>
        <li>
          <a href="https://github.com/tahamsi/aegisrag-poe">aegisrag-poe</a><br/>
          Verifiable reasoning and proof-carrying retrieval for long-context LLM workflows.
        </li>
        <li>
          <a href="https://github.com/tahamsi/claim-conditioned-cad">claim-conditioned-cad</a><br/>
          LLM-related research prototype for claim-conditioned workflows.
        </li>
      </ul>
    </td>
    <td valign="top" width="33%">
      <h3>Responsible AI</h3>
      <ul>
        <li>
          <a href="https://github.com/tahamsi/instance-wise-explanation">instance-wise-explanation</a><br/>
          Work on explainability and instance-level interpretation.
        </li>
        <li>
          <a href="https://github.com/tahamsi/fairfactor-temporal-fer">fairfactor-temporal-fer</a><br/>
          Fairness-aware temporal facial expression recognition.
        </li>
      </ul>
    </td>
  </tr>
</table>

---

## ğŸ§‘â€ğŸ« Teaching

<table>
  <tr>
    <td valign="top" width="50%">
      <h3>Applied Computer Vision</h3>
      <ul>
        <li>
          <a href="https://github.com/tahamsi/computer-vision">computer-vision</a><br/>
          Hands-on notebooks and examples for applied computer vision workflows.
        </li>
      </ul>
    </td>
    <td valign="top" width="50%">
      <h3>Labs</h3>
      <ul>
        <li>
          <a href="https://github.com/tahamsi/cnn-transformers-cv-labs">cnn-transformers-cv-labs</a><br/>
          Master's-level labs covering CNNs and Vision Transformers.
        </li>
        <li>
          <a href="https://github.com/tahamsi/trustworthy-ai-playbook">trustworthy-ai-playbook</a><br/>
          Practical lab-ready materials and artefacts for trustworthy AI development.
        </li>
      </ul>
    </td>
  </tr>
</table>

---

## ğŸ“ Supervision

My supervision interests focus on AI, particularly computer vision, LLMs, and multimodal AI, with emphasis on trustworthy and ethically grounded real-world applications.

---

## ğŸ“„ Selected Outputs

- [Inclusive prompt engineering for large language models: a modular framework for ethical, structured, and adaptive AI](https://link.springer.com/article/10.1007/s10462-025-11330-7)
- [A novel triplet loss architecture with visual explanation for detecting the unwanted rotation of bolts in safety-critical environments](https://www.sciencedirect.com/science/article/pii/S095219762501098X)
- [IEEE Xplore publication](https://ieeexplore.ieee.org/abstract/document/9801817)
- [Learning Fuzzy Cognitive Maps with modified asexual reproduction optimisation algorithm](https://www.sciencedirect.com/science/article/abs/pii/S0950705118304854)

---

## ğŸ“« Contact

<p>
  <a href="https://www.salford.ac.uk/our-staff/taha-mansouri">ğŸ« University Profile</a> Â·
  <a href="https://www.linkedin.com/in/taha-mansouri-7969095a/">ğŸ’¼ LinkedIn</a> Â·
  <a href="mailto:t.mansouri@salford.ac.uk">âœ‰ï¸ Email</a>
</p>
