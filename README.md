# Taha Mansouri

Lecturer in AI at the University of Salford  
Computer Vision & Multimodal AI 路 LLMs 路 Responsible AI

<p align="left">
  <img src="https://img.shields.io/badge/Computer%20Vision-%26%20Multimodal%20AI-1f6feb" alt="Computer Vision and Multimodal AI" />
  <img src="https://img.shields.io/badge/LLMs-Reasoning%20%26%20RAG-8250df" alt="LLMs" />
  <img src="https://img.shields.io/badge/Responsible%20AI-Fairness%20%26%20Explainability-0e8a16" alt="Responsible AI" />
  <img src="https://img.shields.io/badge/University%20of%20Salford-Lecturer%20in%20AI-7A003C" alt="University of Salford" />
</p>

[University Profile](https://www.salford.ac.uk/our-staff/taha-mansouri) 路 [LinkedIn](https://www.linkedin.com/in/taha-mansouri-7969095a/) 路 [GitHub](https://github.com/tahamsi)

---

## About

I am a Lecturer in Artificial Intelligence at the University of Salford, working across research, teaching, supervision, and applied AI projects.

My work focuses on Computer Vision & Multimodal AI, LLMs, and Responsible AI, with an emphasis on systems that are explainable, robust, and suitable for real-world deployment.

I combine academic research with industry-facing work, and I am particularly interested in trustworthy AI evaluation, multimodal systems, and practical learning resources for AI education.

---

## Research Interests

<table>
  <tr>
    <td valign="top" width="33%">
      <h3>Computer Vision & Multimodal AI</h3>
      <ul>
        <li>Visual understanding and perception pipelines</li>
        <li>Vision-language systems and multimodal integration</li>
        <li>Applied CV for real-world settings</li>
      </ul>
    </td>
    <td valign="top" width="33%">
      <h3>LLMs</h3>
      <ul>
        <li>RAG and orchestration patterns</li>
        <li>Verifiable reasoning and evaluation</li>
        <li>Applied LLM systems for domain tasks</li>
      </ul>
    </td>
    <td valign="top" width="33%">
      <h3>Responsible AI</h3>
      <ul>
        <li>Fairness, bias, and robustness</li>
        <li>Explainability and transparency</li>
        <li>Governance, auditing, and safe deployment</li>
      </ul>
    </td>
  </tr>
</table>

---

## Research

<table>
  <tr>
    <td valign="top" width="33%">
      <h3>Computer Vision & Multimodal AI</h3>
      <ul>
        <li>
          <a href="https://github.com/tahamsi/facial_expression_detection">facial_expression_detection</a><br/>
          Video-based facial expression analysis and model-checking workflows.
        </li>
        <li>
          <a href="https://github.com/tahamsi/qwen-vl-caption-api">qwen-vl-caption-api</a><br/>
          Vision-language captioning experiments and API integration work.
        </li>
      </ul>
    </td>
    <td valign="top" width="33%">
      <h3>LLMs</h3>
      <ul>
        <li>
          <a href="https://github.com/tahamsi/aegisrag-poe">aegisrag-poe</a><br/>
          Verifiable reasoning and proof-carrying retrieval for long-context LLM workflows.
        </li>
        <li>
          <a href="https://github.com/tahamsi/stateful_rag_orchestrator">stateful_rag_orchestrator</a><br/>
          Stateful orchestration patterns for retrieval-augmented systems.
        </li>
      </ul>
    </td>
    <td valign="top" width="33%">
      <h3>Responsible AI</h3>
      <ul>
        <li>
          <a href="https://github.com/tahamsi/trustworthy-ai-playbook">trustworthy-ai-playbook</a><br/>
          Practical artefacts and guidance for trustworthy AI development.
        </li>
        <li>
          <a href="https://github.com/tahamsi/fairfactor-temporal-fer">fairfactor-temporal-fer</a><br/>
          Fairness-aware temporal facial expression recognition.
        </li>
      </ul>
    </td>
  </tr>
</table>

---

## Teaching

<table>
  <tr>
    <td valign="top" width="50%">
      <h3>Computer Vision Labs</h3>
      <ul>
        <li>
          <a href="https://github.com/tahamsi/cnn-transformers-cv-labs">cnn-transformers-cv-labs</a><br/>
          Master's-level labs covering CNNs and Vision Transformers.
        </li>
      </ul>
    </td>
    <td valign="top" width="50%">
      <h3>Practical Examples</h3>
      <ul>
        <li>
          <a href="https://github.com/tahamsi/computer-vision">computer-vision</a><br/>
          Hands-on notebooks and examples for applied computer vision workflows.
        </li>
      </ul>
    </td>
  </tr>
</table>

---

## Supervision

I welcome PhD enquiries and research collaboration in the following areas:

<table>
  <tr>
    <td valign="top" width="33%">
      <h3>Computer Vision & Multimodal AI</h3>
      <ul>
        <li>Explainable computer vision</li>
        <li>Vision-language modelling</li>
        <li>Fairness in visual systems</li>
      </ul>
    </td>
    <td valign="top" width="33%">
      <h3>LLMs</h3>
      <ul>
        <li>Trustworthy LLM evaluation</li>
        <li>RAG and auditable pipelines</li>
        <li>Agentic AI for real-world tasks</li>
      </ul>
    </td>
    <td valign="top" width="33%">
      <h3>Responsible AI</h3>
      <ul>
        <li>AI governance and auditing</li>
        <li>Bias, robustness, and transparency</li>
        <li>Deployment-focused responsible AI</li>
      </ul>
    </td>
  </tr>
</table>

---

## Research and Teaching Overview

```mermaid
flowchart LR
    A[Responsible AI] --> D[Applied AI Systems]
    B[LLMs] --> D
    C[Computer Vision & Multimodal AI] --> D
    D --> E[Research Prototypes]
    D --> F[Teaching Labs]
    D --> G[Supervision Topics]
